{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoDistanceEstimationTracking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y20w3Wc6uj-r"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "install requirements:\n",
        "repos: Main Repo, DINO, DPT (with additional weights)\n",
        "\"\"\"\n",
        "!git clone https://github.com/PJ-cs/DistanceEstimationTracking.git\n",
        "\n",
        "\n",
        "!(git clone https://github.com/intel-isl/DPT.git && cd DPT && git checkout f43ef9e08d70a752195028a51be5e1aff227b913)\n",
        "!pip install timm\n",
        "!wget https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\n",
        "!mv dpt_hybrid-midas-501f0c75.pt DPT/weights\n",
        "!mv dpt_large-midas-2f21e586.pt DPT/weights\n",
        "\n",
        "!(git clone https://github.com/aim-uofa/AdelaiDepth.git && cd AdelaiDepth && git checkout 35ddf7e86c7d1328fe6f45a5b7f9633b0544aab8)\n",
        "!pip install -r AdelaiDepth/LeReS/requirements.txt\n",
        "!apt-get install libsparsehash-dev\n",
        "!pip install --upgrade git+https://github.com/mit-han-lab/torchsparse.git@e268836e64513b9a31c091cd1d517778d4c1b9e6\n",
        "\n",
        "\n",
        "!(git clone https://github.com/facebookresearch/dino.git && cd dino && git checkout cb711401860da580817918b9167ed73e3eef3dcf)\n",
        "!wget https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth\n",
        "!mv dino_deitsmall8_pretrain.pth dino\n",
        "\n",
        "!pip install filterpy\n",
        "!pip install lap\n",
        "\n",
        "!mv DistanceEstimationTracking/dataset.py .\n",
        "!mv DistanceEstimationTracking/models.py .\n",
        "!mv DistanceEstimationTracking/sort_2_5D.py .\n",
        "!mv -f DistanceEstimationTracking/run_monodepth.py DPT/\n",
        "!mv -f DistanceEstimationTracking/video_generation.py dino/\n",
        "\n",
        "!mkdir Images\n",
        "\n",
        "#!mkdir Images/S01_color\n",
        "!unzip DistanceEstimationTracking/S01_color.zip -d Images\n",
        "!mv Images/color Images/S01_color\n",
        "!rm Images/S01_color/{000001..000030}.jpg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# download megadetector\n",
        "(git clone https://github.com/Microsoft/CameraTraps && cd CameraTraps && git checkout v5.0)\n",
        "(git clone https://github.com/Microsoft/ai4eutils && cd ai4eutils && git checkout 9260e6b876fd40e9aecac31d38a86fe8ade52dfd)\n",
        "(git clone https://github.com/ultralytics/yolov5 && cd yolov5 && git checkout c23a441c9df7ca9b1f275e8c8719c949269160d1)\n",
        "wget -q -O CameraTraps/detection/md_v5a.0.0.pt https://github.com/microsoft/CameraTraps/releases/download/v5.0/md_v5a.0.0.pt\n",
        "pip install virtualenv\n",
        "virtualenv cameratraps-detector\n",
        "bash -c \"source /content/cameratraps-detector/bin/activate && pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html\"\n",
        "bash -c \"source /content/cameratraps-detector/bin/activate && pip install numpy pandas tqdm opencv-python requests jsonpickle 'Pillow==9.1.0' humanfriendly matplotlib 'PyYAML>=5.3.1' 'seaborn>=0.11.0'\""
      ],
      "metadata": {
        "id": "Ud6NkcFSkMi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import argparse\n",
        "\n",
        "import cv2\n",
        "import os \n",
        "import json\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageFile, ImageFont, ImageDraw\n",
        "import shutil\n",
        "from sort_2_5D import Sort2_5D, KalmanBoxTracker\n",
        "import glob\n",
        "from models import SPVCNN_CLASSIFICATION\n",
        "import torch\n",
        "from dataset import *\n",
        "from torchsparse.utils.helpers import sparse_collate_tensors\n",
        "from collections import OrderedDict\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"DPT\")\n",
        "import DPT.run_monodepth as run_dpt_depth\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"HYPERPARAMETERS\"\"\"\n",
        "ALPHA_IOU = 0.4270 # ! >0 [0, 1]\n",
        "#BETA_DISTZ = 0.5 # ! >0   = 1 - alpha_iou\n",
        "MAX_DIST = 4.0962 # [m]\n",
        "IOU_THRES = 0.0101\n",
        "MAX_AGE = 111\n",
        "MIN_HITS = 1\n",
        "DET_CONF_THRES = 0.9160973480326474 # 0.9\n",
        "DET_CLASSES = {1}  # or {1, 2} to also detect humans\n",
        "\n",
        "PERCENTILE = 50\n",
        "DINO_THRESH = 26 # [0, 255]\n",
        "DINO_RES = 256 # or 512\n",
        "\n",
        "def dino_semseg(rgb_dir, output_dir, threshold = DINO_THRESH):\n",
        "    # output_dir must be empty, mask-original_file_name in output dir\n",
        "    #os.system(f'python dino/video_generation.py --pretrained_weights dino_deitsmall8_pretrain.pth --input_path \"{rgb_dir}\" --output_path \"{output_dir}\" --resize 512 ')\n",
        "    %run dino/video_generation.py --pretrained_weights \"dino/dino_deitsmall8_pretrain.pth\" --input_path $rgb_dir --output_path $output_dir --resize $threshold\n",
        "    attn_dir = os.path.join(output_dir, \"attention\")\n",
        "    \n",
        "\n",
        "    # delete unnecessary video\n",
        "    #os.remove(os.path.join(output_dir, \"video.mp4\"))\n",
        "    # create binary masks of images, names: mask-original_file_name and resize to original res\n",
        "    for rgb_img in os.scandir(rgb_dir):\n",
        "        if rgb_img.is_file() and (rgb_img.name.lower().endswith(\".jpg\") or rgb_img.name.lower().endswith(\".jpeg\") or rgb_img.name.lower().endswith(\".png\")):\n",
        "            # open att_img and get original shape\n",
        "            rgb_shape = cv2.imread(rgb_img.path).shape[:2]\n",
        "            att_img_path = os.path.join(attn_dir, \"attn-\"+rgb_img.name)\n",
        "            att_img_file = cv2.imread(att_img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # resize att img to orignal dims\n",
        "            att_img_res = cv2.resize(att_img_file, (rgb_shape[1], rgb_shape[0]))\n",
        "            # create binary mask\n",
        "            att_img_res[att_img_res <= threshold] = 0\n",
        "            att_img_res[att_img_res > threshold] = 255\n",
        "            # save mask\n",
        "            cv2.imwrite(os.path.join(output_dir, \"mask-\"+rgb_img.name[:-3]+\"png\"), att_img_res)\n",
        "            \n",
        "    # delete attention dir\n",
        "    shutil.rmtree(attn_dir) \n",
        "\n",
        "\"\"\"inference notebook\"\"\"\n",
        "\n",
        "\n",
        "# TODO add later: argparse for these arguments and change focal_length calculation\n",
        "input_frames_dir = \"Images/S01_color\"\n",
        "\n",
        "input_fov_deg = 89.89943662633006\n",
        "algn_out_dir = \"inference_test/algn_out\"\n",
        "tracks_out_dir = \"inference_test\"\n",
        "\n",
        "mega_det_onnx_path = \"DeepChimpact/weights/md_v4.1.0.onnx\"\n",
        "pvcnn_weights_path = \"DistanceEstimationTracking/align_weights.pth\"\n",
        "dpt_weights_path = \"DPT/weights/dpt_large-midas-2f21e586.pt\"\n",
        "\n",
        "single_imgs = False\n",
        "\n",
        "# end argparse\n",
        "\n",
        "crops_temp_folder = \"temp/crops\"\n",
        "masks_temp_folder = \"temp/masks\"\n",
        "dpt_temp_folder = \"temp/dpt\"\n",
        "# detections_temp_folder = \"temp/detections\"\n",
        "tracks_out_path = os.path.join(tracks_out_dir, os.path.basename(input_frames_dir)+\"_output.csv\")\n",
        "img_height = 0\n",
        "img_width = 0\n",
        "\n",
        "os.makedirs(crops_temp_folder, exist_ok=True)\n",
        "os.makedirs(masks_temp_folder, exist_ok=True)\n",
        "os.makedirs(dpt_temp_folder, exist_ok=True)\n",
        "os.makedirs(algn_out_dir, exist_ok=True)\n",
        "\n",
        "# get img_height, img_width\n",
        "for rgb_img in os.scandir(input_frames_dir):\n",
        "    if rgb_img.is_file() and rgb_img.name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "        test_img = cv2.imread(rgb_img.path)\n",
        "        img_height, img_width = test_img.shape[:2]\n",
        "        break\n",
        "\n",
        "input_focal_length_px = (img_width * 0.5) / math.tan(input_fov_deg * 0.5 * math.pi / 180. )\n",
        "print(input_focal_length_px)\n",
        "# input_focal_lenght_px = 424.7448425292969\n",
        "\n",
        "print(f\"1: Calculating DPT images, saving to {dpt_temp_folder} ...\")\n",
        "run_dpt_depth.run(input_frames_dir,\n",
        "                  dpt_temp_folder,\n",
        "                  dpt_weights_path,\n",
        "                  \"dpt_large\")\n",
        "\n",
        "print(f\"2: Converting Relative Depth images to absolute images via PVCNN, saving results to {algn_out_dir}...\")\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "voxel_size=0.01\n",
        "num_points=50000\n",
        "spvcnn_model = SPVCNN_CLASSIFICATION(input_channel=3, num_classes=2, cr=1.0, pres=voxel_size, vres=voxel_size)\n",
        "checkpoint = torch.load(pvcnn_weights_path, map_location=device)\n",
        "spvcnn_model.load_state_dict(checkpoint['spvcnn_model_state_dict'])\n",
        "\n",
        "# move model to device\n",
        "spvcnn_model.to(device)\n",
        "spvcnn_model.eval()\n",
        "\n",
        "# transforms, datasets, dataloader\n",
        "dpt_transforms = get_transforms_dpt(voxel_size, num_points)\n",
        "\n",
        "img_paths = glob.glob(os.path.join(dpt_temp_folder, \"*.pfm\"))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for dpt_img_file in tqdm(img_paths):\n",
        "\n",
        "        dpt_img = cv2.imread(dpt_img_file, cv2.IMREAD_UNCHANGED)\n",
        "        dpt_img_name = os.path.basename(dpt_img_file)\n",
        "        # transform dpt desparity to relative depth\n",
        "        dpt_pcd = dpt_img.copy()\n",
        "\n",
        "        dpt_pcd -= dpt_pcd.min()\n",
        "        dpt_pcd /= dpt_pcd.max()\n",
        "        dpt_pcd = 1./(dpt_pcd*0.5+0.02)\n",
        "        dpt_pcd_tensor = torch.from_numpy(dpt_pcd).unsqueeze(0)\n",
        "        \n",
        "        dpt_shape = tuple(dpt_pcd_tensor.shape[-2:])\n",
        "        gt_shape = (img_height, img_width)\n",
        "        \n",
        "        if dpt_shape != gt_shape:\n",
        "          dpt_pcd_tensor =torch.nn.functional.interpolate(\n",
        "                          dpt_pcd_tensor.unsqueeze(0),\n",
        "                          size=gt_shape,\n",
        "                          mode=\"bicubic\",\n",
        "                          align_corners=False,).squeeze(0)\n",
        "            \n",
        "        # transform dpt img to pointcloud\n",
        "        dpt_sparse, dpt_normalized = dpt_transforms((dpt_pcd_tensor, input_focal_length_px))\n",
        "        dpt_sparse_input = sparse_collate_tensors([dpt_sparse]).to(device)\n",
        "\n",
        "        # inference\n",
        "        model_out = spvcnn_model(dpt_sparse_input)\n",
        "        scale_out = model_out[:,0]\n",
        "        shift_out = model_out[:,1]\n",
        "\n",
        "        # align depth image with output\n",
        "        dpt_aligned = dpt_pcd_tensor.squeeze(0).squeeze(0).cpu().numpy() * scale_out[0].cpu().numpy() + shift_out[0].cpu().numpy()\n",
        "\n",
        "        # save output\n",
        "        cv2.imwrite(os.path.join(algn_out_dir, dpt_img_name), dpt_aligned)\n",
        "\n",
        "\n",
        "print(f\"3: Calculating Detections, saving crops to {crops_temp_folder}...\")\n",
        "! bash -c 'source /content/cameratraps-detector/bin/activate && PYTHONPATH=\"$PYTHONPATH:CameraTraps:ai4eutils:yolov5\" python CameraTraps/detection/run_detector_batch.py CameraTraps/detection/md_v5a.0.0.pt Images detections.json --recursive'\n",
        "with open(\"detections.json\") as f:\n",
        "  detections = json.load(f)\n",
        "detections_by_image_path = {\n",
        "    image[\"file\"]: image for image in detections[\"images\"] if input_frames_dir in image[\"file\"]\n",
        "}\n",
        "\n",
        "frame_det_dict = {}\n",
        "\n",
        "\n",
        "for rgb_img_path in tqdm(detections_by_image_path.keys()):\n",
        "    frame_name = os.path.basename(rgb_img_path)\n",
        "    frame_id = os.path.splitext(frame_name)[0]\n",
        "\n",
        "    input_pil = Image.open(rgb_img_path)\n",
        "    input_cv = cv2.imread(rgb_img_path)\n",
        "    frame_height = input_pil.height\n",
        "    frame_width = input_pil.width\n",
        "    \n",
        "    frame_det_dict[frame_name] = {}\n",
        "    \n",
        "    \n",
        "    for det_ind, detection in enumerate(detections_by_image_path[rgb_img_path][\"detections\"]):\n",
        "        if int(detection[\"category\"]) not in DET_CLASSES or detection[\"conf\"] < DET_CONF_THRES:\n",
        "            continue\n",
        "        bb = detection[\"bbox\"]\n",
        "        bbx = int(bb[0] * frame_width)\n",
        "        bby = int(bb[1] * frame_height)\n",
        "        bbwidth = int(bb[2] * frame_width)\n",
        "        bbheight = int(bb[3] * frame_height)\n",
        "        # print(bbx, bby, bbwidth, bbheight\n",
        "\n",
        "        # new bb\n",
        "        # egde cases, want to guarantee new bb with double the old size\n",
        "        bbx_buffer = bbx - (bbwidth // 2) if bbx - (bbwidth // 2) >= 0 else 0\n",
        "        bbwidth_buffer = 2 * bbwidth \n",
        "        if bbx_buffer + bbwidth_buffer >= frame_width: # move bbx to the left by amount of difference over allowed width\n",
        "\n",
        "            bbx_buffer = frame_width- bbwidth_buffer \n",
        "\n",
        "            if bbx_buffer < 0:\n",
        "                bbx_buffer = 0\n",
        "                bb_width_buffer = frame_width\n",
        "\n",
        "        bby_buffer = bby - (bbheight // 2) if bby - (bbheight // 2) >= 0 else 0\n",
        "        bbheight_buffer = 2 * bbheight \n",
        "        if bby_buffer + bbheight_buffer >= frame_height:\n",
        "            bby_buffer = frame_height - bbheight_buffer \n",
        "            if bby_buffer < 0:\n",
        "                bby_buffer = 0\n",
        "                bb_height_buffer = frame_height\n",
        "\n",
        "        img_det_part = np.copy(input_cv[bby_buffer: bby_buffer + bbheight_buffer, bbx_buffer: bbx_buffer + bbwidth_buffer])\n",
        "\n",
        "\n",
        "        # save to crop_folder\n",
        "\n",
        "        #print(os.path.join(crop_folder, frame_id+f\"_{det_ind:04d}.png\"))\n",
        "\n",
        "        bbx_crop = bbx - bbx_buffer\n",
        "        bby_crop = bby - bby_buffer\n",
        "        # reuse bbwidht, bbheight when extracting depth\n",
        "        frame_det_dict[frame_name][det_ind] = [(bbx, bby, bbwidth, bbheight), (bbx_crop, bby_crop)]\n",
        "\n",
        "        #assert np.all(img_det_part[bby_crop: bby_crop + bbheight, bbx_crop: bbx_crop + bbwidth] == frame_img[bby: bby + bbheight, bbx: bbx + bbwidth])\n",
        "        img_det_part -= img_det_part.min()\n",
        "        img_det_part *= int(255/img_det_part.max())\n",
        "\n",
        "        cv2.imwrite(os.path.join(crops_temp_folder, frame_id+f\"_{det_ind:04d}.png\"), img_det_part)\n",
        "\n",
        "\n",
        "print(f\"4: Starting dino segmentation, saving masks to {masks_temp_folder}...\")\n",
        "dino_semseg(crops_temp_folder, masks_temp_folder)\n",
        "\n",
        "print(f\"5: Extracting distances of detections to camera...\")\n",
        "for frame_name, dets_dict in tqdm(frame_det_dict.items()):\n",
        "    frame_id = os.path.splitext(frame_name)[0]\n",
        "    depth_img = cv2.imread(os.path.join(algn_out_dir, frame_id+\".pfm\"), cv2.IMREAD_UNCHANGED)\n",
        "    frame_height, frame_width = depth_img.shape[:2]\n",
        "    \n",
        "    for det_ind, det_info in (dets_dict.items()):\n",
        "        bbx, bby, bbwidth, bbheight = det_info[0]\n",
        "        bbx_crop, bby_crop = det_info[1]\n",
        "        \n",
        "        # open segmentation mask for detection\n",
        "        seg_det_full = cv2.imread(os.path.join(masks_temp_folder, \"mask-\"+frame_id+f\"_{det_ind:04d}.png\"), cv2.IMREAD_GRAYSCALE) / 255\n",
        "        seg_det_crop = seg_det_full[bby_crop: bby_crop + bbheight, bbx_crop: bbx_crop + bbwidth]\n",
        "        \n",
        "        # get detection crop of depth img\n",
        "        depth_det_crop = depth_img[bby: bby + bbheight, bbx: bbx + bbwidth]\n",
        "        \n",
        "        seg_y, seg_x = np.where((seg_det_crop == 1))[:2]\n",
        "        depth_values_seg = depth_det_crop[seg_y.clip(0, depth_det_crop.shape[0] - 1), seg_x.clip(0, depth_det_crop.shape[1] - 1)]\n",
        "        if (seg_det_crop == 1).any() == False:\n",
        "            print(frame_name, f\"no sem seg pixel of deer in bb {bby},{bbx} dist = {PERCENTILE}th percentile\")\n",
        "            det_info.append(float(np.percentile(depth_det_crop, PERCENTILE)))\n",
        "        else:\n",
        "            det_info.append(float(np.percentile(depth_values_seg, PERCENTILE)))\n",
        "\n",
        "cam_u0 = img_width / 2.0 #848 / 2.0 #frame_depth.shape[1] / 2.0\n",
        "cam_v0 = img_height / 2.0 # 480 / 2.0\n",
        "\n",
        "if single_imgs:\n",
        "  with open(tracks_out_path, 'w', newline='') as csvfile:\n",
        "    fieldnames = ['frame_name', 'bb_x', 'bb_y', 'bb_width', 'bb_height', 'distance', '3D_x', '3D_y', '3D_z']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for frame_name, dets_dict in tqdm(frame_det_dict.items()):\n",
        "      frame_id = os.path.splitext(frame_name)[0]\n",
        "      depth_img = cv2.imread(os.path.join(algn_out_dir, frame_id+\".pfm\"), cv2.IMREAD_UNCHANGED)\n",
        "      frame_height, frame_width = depth_img.shape[:2]\n",
        "      \n",
        "      for det_ind, det_info in (dets_dict.items()):\n",
        "          bbx, bby, bbwidth, bbheight = det_info[0]\n",
        "          distance = det_info[2]\n",
        "\n",
        "          x3d = bbx + 0.5 *bbwidth\n",
        "          y3d = bby + bbheight\n",
        "\n",
        "          # project to 3d\n",
        "          x3d = (x3d-cam_u0) / input_focal_length_px * distance\n",
        "          y3d = (y3d-cam_v0) / input_focal_length_px * distance\n",
        "\n",
        "          writer.writerow({'frame_name': frame_name, 'bb_x':bbx, 'bb_y':bby, 'bb_width':bbwidth, 'bb_height':bbheight, 'distance': distance, '3D_x':x3d, '3D_y':y3d, '3D_z':distance})\n",
        "\n",
        "\n",
        "  print(f\"Finished, saved to: \")\n",
        "\n",
        "else:\n",
        "  print(f\"6: Connecting positions of animals over video to coherent tracks...\")\n",
        "  KalmanBoxTracker.count = 0\n",
        "  # init Sort\n",
        "  mot_tracker = Sort2_5D(max_age=MAX_AGE, min_hits=MIN_HITS, iou_threshold=IOU_THRES, alpha_iou=ALPHA_IOU, max_dist=MAX_DIST)\n",
        "\n",
        "  frame_det_dict = OrderedDict(sorted(frame_det_dict.items(), key=lambda x: abs(int(os.path.splitext(x[0])[0]))))\n",
        "\n",
        "\n",
        "  with open(tracks_out_path, 'w', newline='') as csvfile:\n",
        "      fieldnames = ['frame_name', 'track_num', 'bb_x', 'bb_y', 'bb_width', 'bb_height', 'distance', '3D_x', '3D_y', '3D_z']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "      writer.writeheader()\n",
        "      \n",
        "      for frame_name, dets_dict in tqdm(frame_det_dict.items()):\n",
        "          frame_bbxs = []\n",
        "\n",
        "          for det_ind, det_info in (dets_dict.items()):\n",
        "              bbx, bby, bbwidth, bbheight = det_info[0]\n",
        "              distance = det_info[2]\n",
        "              frame_bbxs.append(np.array([bbx, bby, bbx+bbwidth, bby+bbheight, distance]))\n",
        "          if len(frame_bbxs) == 0: # no detections in frame\n",
        "              frame_bbxs = np.empty((0, 5))\n",
        "          else:\n",
        "              frame_bbxs = np.stack(frame_bbxs, axis=0)\n",
        "\n",
        "          trackers = mot_tracker.update(frame_bbxs)\n",
        "\n",
        "          for d in trackers:\n",
        "              x1,y1,x2, y2,distance, track_num = d\n",
        "              w = x2 -x1\n",
        "              h = y2 -y1\n",
        "              # calculations to project position of animal to 3d\n",
        "              # middle of lower bound of bounding box\n",
        "              x3d = x1 + 0.5 * w\n",
        "              y3d = y1 + h\n",
        "\n",
        "              # project to 3d\n",
        "              x3d = (x3d-cam_u0) / input_focal_length_px * distance\n",
        "              y3d = (y3d-cam_v0) / input_focal_length_px * distance\n",
        "\n",
        "              writer.writerow({'frame_name': frame_name, 'track_num': track_num, 'bb_x':x1, 'bb_y':y1, 'bb_width':w, 'bb_height':h, 'distance': distance, '3D_x':x3d, '3D_y':y3d, '3D_z':distance})\n",
        "  print(f\"finished, saved output to {tracks_out_path}\")"
      ],
      "metadata": {
        "id": "W6jKKyy4xOsg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}